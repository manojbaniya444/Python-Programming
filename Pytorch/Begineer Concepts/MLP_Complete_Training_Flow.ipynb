{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a MultiLayer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining a model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs: int, hidden_size: int, num_outputs: int):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            # output layer\n",
    "            torch.nn.Linear(hidden_size, num_outputs)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = NeuralNetwork(5, 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summary of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=3, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### total trainable parameter of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_parameters_model_0 = sum(p.numel() for p in model_0.parameters())\n",
    "\n",
    "total_parameters_model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accessing model each layer and its weight (in_features * no_of_neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3269,  0.0640, -0.2838, -0.4184,  0.2833],\n",
       "        [ 0.1141, -0.2852, -0.4234, -0.2140, -0.3054],\n",
       "        [ 0.1560,  0.0231,  0.4014, -0.2042, -0.0227]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.layers[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accessing model layer bias (b_size = no_of_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1576,  0.0547, -0.0819], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.layers[0].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0446, 0.9921, 0.4155, 0.5430, 0.9817],\n",
       "        [0.6187, 0.7089, 0.8644, 0.2504, 0.7849],\n",
       "        [0.8849, 0.4689, 0.7500, 0.8020, 0.4507]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((3, 5)) # because 5 is our input feature \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5175,  0.0748],\n",
       "        [-0.5160,  0.0891],\n",
       "        [-0.5160,  0.0889]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model_0(X)\n",
    "\n",
    "out # out is 2 prediction and for 3 input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### no grad\n",
    "If we just want to use the model for prediction after `training or backpropagation` it is expensive to make that `computational graph` which is used for `backpropagation` so we use context manager while inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5175,  0.0748],\n",
       "        [-0.5160,  0.0891],\n",
       "        [-0.5160,  0.0889]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model_0(X)\n",
    "    \n",
    "out # here there is not grad_fn for backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using softmax to get the prob from war logit output of inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3561, 0.6439],\n",
      "        [0.3532, 0.6468],\n",
      "        [0.3532, 0.6468]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out= torch.softmax(model_0(X), dim=1)\n",
    "    \n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Efficient Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6]\n",
    "])\n",
    "y_test = torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class label numbering\n",
    "Our task has two class 0 and 1 which is `Binary Classification` and if there were four classes `0 1 2 3` there should be four output neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining a custom dataset class for dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        one_x = self.features[index]\n",
    "        one_y = self.labels[index]\n",
    "        return one_x, one_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ToyDataset(X_train, y_train)\n",
    "test_dataset = ToyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.2000,  3.1000]), tensor(0))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.8000,  2.8000]), tensor(0))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also use `num_workers` in **DataLoader** to load the next batch of data when the model is training so `CPU` and `GPU` work together to load and train data in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1************************************************** \n",
      "Feature: tensor([[-1.2000,  3.1000],\n",
      "        [ 2.3000, -1.1000]])\n",
      "Label: tensor([0, 1]) with\n",
      "Batch shape: torch.Size([2, 2]) torch.Size([2])\n",
      "\n",
      "Batch 2************************************************** \n",
      "Feature: tensor([[-0.9000,  2.9000],\n",
      "        [-0.5000,  2.6000]])\n",
      "Label: tensor([0, 0]) with\n",
      "Batch shape: torch.Size([2, 2]) torch.Size([2])\n",
      "\n",
      "Batch 3************************************************** \n",
      "Feature: tensor([[ 2.7000, -1.5000]])\n",
      "Label: tensor([1]) with\n",
      "Batch shape: torch.Size([1, 2]) torch.Size([1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(train_dataloader):\n",
    "    print(f\"Batch {i+1}************************************************** \\nFeature: {x}\\nLabel: {y} with\")\n",
    "    print(f\"Batch shape: {x.shape} {y.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# criterion_ = F.cross_entropy(logits, labels) # can also do by this but instantly need to provide logit and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(num_inputs=2, hidden_size=4, num_outputs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5517],\n",
       "        [0.6691]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid(torch.rand(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1\n",
      "Generalization Error: 0.4405747056007385\n",
      "Epoch 1 Batch 2\n",
      "Generalization Error: 0.8143681287765503\n",
      "Epoch 1 Batch 3\n",
      "Generalization Error: 1.177321434020996\n",
      "Epoch 2 Batch 1\n",
      "Generalization Error: 0.8143681287765503\n",
      "Epoch 2 Batch 2\n",
      "Generalization Error: 0.8084791898727417\n",
      "Epoch 2 Batch 3\n",
      "Generalization Error: 0.4415125250816345\n",
      "Epoch 3 Batch 1\n",
      "Generalization Error: 0.8206744194030762\n",
      "Epoch 3 Batch 2\n",
      "Generalization Error: 0.4342684745788574\n",
      "Epoch 3 Batch 3\n",
      "Generalization Error: 1.177321434020996\n",
      "Epoch 4 Batch 1\n",
      "Generalization Error: 0.8094170093536377\n",
      "Epoch 4 Batch 2\n",
      "Generalization Error: 0.4342684745788574\n",
      "Epoch 4 Batch 3\n",
      "Generalization Error: 1.199836254119873\n",
      "Epoch 5 Batch 1\n",
      "Generalization Error: 0.8206744194030762\n",
      "Epoch 5 Batch 2\n",
      "Generalization Error: 0.4342684745788574\n",
      "Epoch 5 Batch 3\n",
      "Generalization Error: 1.177321434020996\n",
      "Epoch 6 Batch 1\n",
      "Generalization Error: 0.4405747056007385\n",
      "Epoch 6 Batch 2\n",
      "Generalization Error: 0.8031107187271118\n",
      "Epoch 6 Batch 3\n",
      "Generalization Error: 1.199836254119873\n",
      "Epoch 7 Batch 1\n",
      "Generalization Error: 0.4352062940597534\n",
      "Epoch 7 Batch 2\n",
      "Generalization Error: 0.8084791898727417\n",
      "Epoch 7 Batch 3\n",
      "Generalization Error: 1.199836254119873\n",
      "Epoch 8 Batch 1\n",
      "Generalization Error: 0.8084791898727417\n",
      "Epoch 8 Batch 2\n",
      "Generalization Error: 0.4352062940597534\n",
      "Epoch 8 Batch 3\n",
      "Generalization Error: 1.199836254119873\n",
      "Epoch 9 Batch 1\n",
      "Generalization Error: 0.8143681287765503\n",
      "Epoch 9 Batch 2\n",
      "Generalization Error: 0.8094170093536377\n",
      "Epoch 9 Batch 3\n",
      "Generalization Error: 0.4396369159221649\n",
      "Epoch 10 Batch 1\n",
      "Generalization Error: 0.8094170093536377\n",
      "Epoch 10 Batch 2\n",
      "Generalization Error: 0.4342684745788574\n",
      "Epoch 10 Batch 3\n",
      "Generalization Error: 1.199836254119873\n",
      "Total time updated weight: 30\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() # training mode\n",
    "    for i, (X, y) in enumerate(train_dataloader):\n",
    "        logits = model(X) # (2, 2) (batch size, features no)\n",
    "        \n",
    "        error = criterion(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        error.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        count = count + 1\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} Batch {i + 1}\")\n",
    "        print(f\"Generalization Error: {error}\")\n",
    "        \n",
    "print(f\"Total time updated weight: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
